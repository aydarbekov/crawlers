# Scraper for the OCCRP web site.
# The goal is not to download all HTML, but only PDFs & other documents
# linked from the page as proof.
name: 24kg_headers

# A title for display in the UI:
description: 'Headers from articles on 24.kg'

# Uncomment to run this scraper automatically:
schedule: monthly
delay: 1
stealthy: true
pipeline:

  init:
    # This first stage will get the ball rolling with a seed URL.
    method: seed
    params:
      urls:
        - https://24.kg/
    handle:
      pass: fetch

  fetch:
    # Download the seed page
    method: fetch
    params:
      # These rules specify which pages should be scraped or included:
      rules:
        or:
          - pattern: 'https://24.kg/vlast/'
          - pattern: 'https://24.kg/obschestvo/'
          - pattern: 'https://24.kg/obschestvo/'
          - pattern: 'https://24.kg/ekonomika/'
          - pattern: 'https://24.kg/proisshestvija/'
          - pattern: 'https://24.kg/agent_024/'
          - pattern: 'https://24.kg/sport/'
          - pattern: 'https://24.kg/tehnoblog/'
          - pattern: 'https://24.kg/kyrgyzcha/'
          - pattern: 'https://24.kg/english/'
          - pattern: 'https://24.kg/biznes_info/'
          - pattern: 'https://24.kg/$'
                    
    handle:
      pass: parse

  parse:
    # Parse the scraped pages to find if they contain additional links.
    method: parse
    params:
      store:
        and:
          - pattern: 'https://24.kg/vlast/'
          - pattern: 'https://24.kg/obschestvo/'
          - pattern: 'https://24.kg/obschestvo/'
          - pattern: 'https://24.kg/ekonomika/'
          - pattern: 'https://24.kg/proisshestvija/'
          - pattern: 'https://24.kg/agent_024/'
          - pattern: 'https://24.kg/sport/'
          - pattern: 'https://24.kg/tehnoblog/'
          - pattern: 'https://24.kg/kyrgyzcha/'
          - pattern: 'https://24.kg/english/'
          - pattern: 'https://24.kg/biznes_info/'
    handle:
      # this makes it a recursive web crawler:
      store: extractdata
      fetch: fetch
  
  extractdata:
    # Parse the scraped pages to extract useful information
    method: example.24_headers:extractdata
    handle:
      pass: store

  store:
    # Store the crawled documents to a directory
    method: db
    params:
      table: "24kg_headers"
      unique:
        - url